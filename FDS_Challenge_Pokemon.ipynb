{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nbstripout FDS_Challenge_Pokemon.ipynb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# FDS Kaggle Competition: Pokémon Battle Prediction\n",
    "\n",
    "**Objective:** This notebook documents the development of a binary classification model to predict the winner of a Pokémon battle using only the data from the first\n",
    "30 turns (for all details, please refer to [FDS Kaggle Competition Pokémon Battles Prediction](https://www.kaggle.com/competitions/fds-pokemon-battles-prediction-2025))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "2ae8cf3e"
   },
   "source": [
    "## Approaches\n",
    "\n",
    "For this project, we considered two different approaches.\n",
    "\n",
    "---\n",
    "\n",
    "### First Approach: Non-Sequential Modeling\n",
    "\n",
    "In the **first approach**, we ignored the sequential nature of the battles. Instead of considering the turn-by-turn evolution, we treated each battle as an independent instance. Using various aggregation functions over the 30 turns, we constructed a single representative record for each battle, capturing overall statistics such as average damage, total HP difference, and key move frequencies. This aggregated representation was then used with traditional machine learning models to predict the final outcome of the battle.\n",
    "\n",
    "---\n",
    "\n",
    "### Second Approach: Sequential Modeling\n",
    "\n",
    "In the **second approach**, we explicitly modeled each battle as a sequence of turns rather than a single static snapshot. Every battle contains up to 30 turns, so we extracted a feature vector at every turn, capturing static team information, HP levels, fainted counts, statuses, boosts, and other dynamic combat signals. This produced a fixed-length sequence representation for each battle, with padding applied to shorter matches.\n",
    "\n",
    "We then trained LSTM and GRU models to learn temporal patterns across turns. To determine the best architecture, we performed a 5-fold cross-validation hyperparameter search, tuning the learning rate, hidden size, bidirectionality, number of recurrent layers, dropout rate, fully connected layer size, and number of training epochs. Each configuration was evaluated using mean validation loss curves, and the best settings were selected accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "cdbf23cc"
   },
   "source": [
    "### 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "faa2adc0"
   },
   "source": [
    "#### 1.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "1ad425a5"
   },
   "outputs": [],
   "source": [
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "26f14342"
   },
   "source": [
    "#### 1.2. Kaggle Setup & Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "referenced_widgets": [
      "53fe34240463401b9d494c3a80fd92fa",
      "e6eaac1d924a4d11990648c8d888e6ab",
      "75c158b0674947619bb9593994cdf011",
      "421c8ddc082b4c76bab3ad36aafc62a0",
      "a5b26a10b1b34b00978c2a5892bd1841"
     ]
    },
    "id": "d66e6270",
    "outputId": "bea21fb6-d2c9-4a58-a917-9bcef1745044"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
    "print(\"Logging into Kaggle...\")\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57c7a3d1",
    "outputId": "11b91b12-dfd1-4e11-d841-e3494483384a"
   },
   "outputs": [],
   "source": [
    "print(\"\\nDownloading competition data...\")\n",
    "# This helper function finds the path to the competition data\n",
    "fds_pokemon_battles_prediction_2025_path = kagglehub.competition_download('fds-pokemon-battles-prediction-2025')\n",
    "print('Data source import complete.')\n",
    "\n",
    "# --- Define the path to our data ---\n",
    "DATA_PATH = fds_pokemon_battles_prediction_2025_path\n",
    "print(f\"Data path set to: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "6f5bfa18"
   },
   "source": [
    "### 1.3. Load JSONL Data\n",
    "\n",
    "We read each file line-by-line and store the data in two lists: `train_data` and `test_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1280703",
    "outputId": "d3e50a68-299b-4929-d5b4-86d2051dc4c2"
   },
   "outputs": [],
   "source": [
    "from utils.data import load_jsonl_data\n",
    "\n",
    "# --- Load Training Data ---\n",
    "train_file_path = os.path.join(DATA_PATH, 'train.jsonl')\n",
    "train_data = load_jsonl_data(train_file_path)\n",
    "\n",
    "# --- Load Testing Data ---\n",
    "test_file_path = os.path.join(DATA_PATH, 'test.jsonl')\n",
    "test_data = load_jsonl_data(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "d299cce4"
   },
   "source": [
    "### 2. Exploratory Data Analysis (EDA) & Domain Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "618e409b"
   },
   "source": [
    "#### 2.1. First Look: The Anatomy of a Battle\n",
    "\n",
    "We inspect the first record from our training data (`train_data[0]`) to understand its nested keys and complex structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdec5056",
    "outputId": "e9cfbdf2-c3a2-42bc-b425-f5bfd99aaa5c"
   },
   "outputs": [],
   "source": [
    "print(json.dumps(train_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "b43d228e"
   },
   "source": [
    "#### 2.2. Data Structure Overview\n",
    "\n",
    "Our first look at `train_data[0]` reveals that each battle is a complex JSON object with several root keys. The **root-level keys** for each battle object are::\n",
    "\n",
    "* `\"player_won\"`: Our boolean target variable.\n",
    "* `\"battle_id\"`: A unique identifier for the battle.\n",
    "* `\"p1_team_details\"`: A **list of 6** dictionaries, one for each Pokémon on Player 1's team. This includes their full base stats (e.g., `base_hp`, `base_atk`).\n",
    "* `\"p2_lead_details\"`: A **single dictionary** describing only Player 2's *lead* Pokémon. This also includes its base stats.\n",
    "* `\"battle_timeline\"`: A list of dictionaries, one for each turn (up to 30), detailing the in-battle state, moves used, and active Pokémon for both players.\n",
    "\n",
    "This structure shows we have full team information for Player 1, but only partial \"Turn 0\" information for Player 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "id": "587a99bf"
   },
   "source": [
    "#### 2.3. Full Data Scan & Knowledge Base Creation\n",
    "\n",
    "To understand the full scope of the data, we will perform one comprehensive pass over the entire `train_data`.\n",
    "\n",
    "The goals of this scan are:\n",
    "1.  **Discover all unique game elements**: Find every unique status (e.g., 'brn', 'par'), effect, move, etc. This is essential for creating a complete Data Dictionary.\n",
    "2.  **Build a Pokémon Database (`pokemon_db`)**: As a useful new representation of the data, we will store the base stats for every Pokémon species we encounter. This will allow us to look up stats for any Pokémon seen in the timeline, not just P1's team or P2's lead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389,
     "referenced_widgets": [
      "ea41f652dfe545e8800dbe8d13b801f3",
      "3505763abc7447a0abe95313ceec7220",
      "81cdb69f7e294db3ad39031efd1a7716",
      "750b7f8a1f344be296dcbadab7a05456",
      "802a9b53d4664260b3d50712667071b4",
      "8fc1101e4b334bfabd616b3a5af73dd7",
      "7ad79daa5ba74b4eb48c4b71406ab7a0",
      "0b7cfe87e3014a508942cd9338ad4b9b",
      "b0a033bd486348c5a62a1c84879f8ee4",
      "5f186dc9bbce4882a3a207be4cfc25cb",
      "87c6103130074edbb9b1ea8c3d4098d2"
     ]
    },
    "id": "402f22d1",
    "outputId": "dfc68520-d2bc-4f91-8adf-ad56d2a02f72"
   },
   "outputs": [],
   "source": [
    "from utils.pokemon import build_knowledge_base, print_universe_summary\n",
    "\n",
    "pokemon_db, unique_game_elements = build_knowledge_base(train_data)\n",
    "print_universe_summary(pokemon_db, unique_game_elements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "id": "0a029956"
   },
   "source": [
    "#### 2.4. The Data Dictionary\n",
    "\n",
    "We can now define our official **Data Dictionary**. This documents the key data fields discovered.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Root Level Keys**\n",
    "\n",
    "* `\"player_won\"`: Our **target variable**. Boolean (`true`/`false`).\n",
    "* `\"battle_id\"`: The unique **identifier** for the battle.\n",
    "* `\"p1_team_details\"`: A list of 6 dictionaries (one for each Pokémon) containing their static details. (See `Pokémon Details` below).\n",
    "* `\"p2_lead_details\"`: A single dictionary for P2's lead Pokémon. (See `Pokémon Details` below).\n",
    "* `\"battle_timeline\"`: A list of turn objects (up to 30), representing the battle's progression.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Level 2: Pokémon Details (Static)**\n",
    "*(Found within `p1_team_details` and `p2_lead_details`)*\n",
    "\n",
    "Describes a Pokémon's intrinsic properties.\n",
    "\n",
    "* `\"name\"`: The species name (e.g., `'alakazam'`, `'snorlax'`).\n",
    "* `\"level\"`: The Pokémon's level (always `100`).\n",
    "* `\"types\"`: A list of elemental types (e.g., `['grass', 'psychic']`).\n",
    "* **Base Stats:**\n",
    "    * `\"base_hp\"`: Base Hit Points (durability).\n",
    "    * `\"base_atk\"`: Base Attack (for Physical moves).\n",
    "    * `\"base_def\"`: Base Defense (against Physical moves).\n",
    "    * `\"base_spa\"`: Base Special Attack (for Special moves).\n",
    "    * `\"base_spd\"`: Base Special Defense (against Special moves).\n",
    "    * `\"base_spe\"`: Base Speed (determines action order).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Level 2: Pokémon State (Dynamic)**\n",
    "*(Found within `battle_timeline` as `pX_pokemon_state`)*\n",
    "\n",
    "Details the active Pokémon's state at the end of a turn.\n",
    "\n",
    "* `\"name\"`: The name of the Pokémon on the field.\n",
    "* `\"hp_pct\"`: Current HP Percentage (Float, 0.0 to 1.0).\n",
    "* `\"status\"`: The main non-volatile status condition.\n",
    "    * **Unique Values:** `['brn', 'fnt', 'frz', 'nostatus', 'par', 'psn', 'slp', 'tox']`\n",
    "    * **Interpretation:** `fnt` (Faint), `brn` (Burn), `par` (Paralysis), `slp` (Sleep), etc.\n",
    "* `\"boosts\"`: Temporary stat multipliers for `atk`, `def`, `spa`, `spd`, `spe`. (Integer, -6 to +6).\n",
    "* `\"effects\"`: Volatile battle effects.\n",
    "    * **Unique Values:** `['clamp', 'confusion', 'firespin', 'noeffect', 'reflect', 'substitute', 'typechange', 'wrap']`\n",
    "\n",
    "---\n",
    "\n",
    "#### **Level 2: Move Details (Dynamic)**\n",
    "*(Found within `battle_timeline` as `pX_move_details`)*\n",
    "\n",
    "Details the move selected in that turn (`null` if none).\n",
    "\n",
    "* `\"name\"`: The name of the move used.\n",
    "    * **Unique Values (40 total):** `['agility', 'amnesia', 'blizzard', 'bodyslam', ... , 'thunderwave', 'toxic', 'wrap']`\n",
    "* `\"category\"`: The move's category.\n",
    "    * **Unique Values:** `['PHYSICAL', 'SPECIAL', 'STATUS']`\n",
    "    * **Interpretation:** Determines damage calculation (Physical/Special) or no damage (Status).\n",
    "* `\"type\"`: The move's elemental type.\n",
    "    * **Unique Values (13 total):** `['ELECTRIC', 'FIGHTING', 'FIRE', ... , 'ROCK', 'WATER']`\n",
    "* `\"base_power\"`: The move's base power (Integer). 0 for Status moves.\n",
    "* `\"accuracy\"`: The move's chance to hit (Float, 0.0-1.0).\n",
    "* `\"priority\"`: Determines move order independent of Speed (Integer, usually 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## First Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "ff990d6c"
   },
   "source": [
    "### 3. Methodology & Reusable Components\n",
    "\n",
    "This section defines the core reusable functions for our pipeline, ensuring consistency across all experiments.\n",
    "\n",
    "1.  **Validation Strategies:** Standardized functions for model evaluation.\n",
    "2.  **Feature Extraction Helpers:** Modular functions to extract specific information groups.\n",
    "3.  **Main Feature Creation Pipeline:** The central orchestrator that builds the final feature set for any given iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "6a237846"
   },
   "source": [
    "#### 3.1. Validation Strategies\n",
    "\n",
    "We define two validation methods:\n",
    "1.  `run_simple_validation`: A basic 90/10 `train_test_split`.\n",
    "2.  `run_kfold_validation`: A robust 5-Fold Cross-Validation. This is our \"gold standard\" for final model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a89c4448",
    "outputId": "16f38398-dcfb-49a6-ec32-682e28d5fcec"
   },
   "outputs": [],
   "source": [
    "from utils.singular.validation import (\n",
    "    run_simple_validation,\n",
    "    run_kfold_validation,\n",
    "    init_results_store,\n",
    "    store_result,\n",
    ")\n",
    "results = init_results_store()\n",
    "\n",
    "print(\"Defined validation functions: 'run_simple_validation' and 'run_kfold_validation'.\")\n",
    "print(\"Initialized 'results' dictionary to store scores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "32f002b4"
   },
   "source": [
    "## 3.2. Feature Extraction Helpers\n",
    "\n",
    "These are our modular \"helper\" functions. Each function is responsible for one logical task (e.g., extracting only the static \"Turn 0\" stats).\n",
    "\n",
    "For Iteration 1, we only need to define the helper that extracts our static (Turn 0) features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "117c3ae6",
    "outputId": "e25e1aa1-42b4-45e3-ddf7-5fbb99446819"
   },
   "outputs": [],
   "source": [
    "from utils.singular.feature import *\n",
    "print(\"Defined all v1, v2, and v3 (REFINED) feature extraction helper functions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "6aa32471"
   },
   "source": [
    "## 3.3. Main Feature Creation Pipeline\n",
    "\n",
    "This is the main function that builds our features. It calls the correct helper functions (from 3.2) based on an `iteration_name`.\n",
    "\n",
    "This centralized approach ensures that the raw data is processed consistently and efficiently for each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88aa1297",
    "outputId": "d532f73e-fb2a-4a8b-947d-f6d922f93a47"
   },
   "outputs": [],
   "source": [
    "from utils.singular.feature import create_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "04f5939e"
   },
   "source": [
    "# 4. Iteration 1: Baseline (Static Snapshot @ Turn 0)\n",
    "\n",
    "**Objective:** Establish our initial baseline score.\n",
    "\n",
    "**Features (v1):** 12 features.\n",
    "* P1: Mean stats of all 6 team members (6 stats).\n",
    "* P2: Base stats of the *single lead* Pokémon (6 stats).\n",
    "\n",
    "**Validation:** Simple 90/10 Split (using our `run_simple_validation` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223,
     "referenced_widgets": [
      "459fb2be45de4db7b1f93a5c73f857f2"
     ]
    },
    "id": "088d16a1",
    "outputId": "445f28e3-1979-4e53-d8e5-56f6011c7cab"
   },
   "outputs": [],
   "source": [
    "# 1. Create v1 features\n",
    "X_v1, y_v1 = create_features(train_data, pokemon_db, \"v1\")\n",
    "\n",
    "# 2. Run v1 validation\n",
    "results['v1_simple_split'] = run_simple_validation(\n",
    "    X_v1,\n",
    "    y_v1,\n",
    "    model_class=LogisticRegression, # Using Logistic Regression as our baseline model\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "print(f\"\\nIteration 1 Baseline Accuracy: {results['v1_simple_split']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "9f3c7999"
   },
   "source": [
    "# 5. Iteration 2: Key Dynamics (Snapshot @ Turn 30)\n",
    "\n",
    "**Objective:** Measure the impact of adding key \"Turn 30 Snapshot\" features.\n",
    "\n",
    "**Features (v2):** 19 features total.\n",
    "\n",
    "* **(v1)** All 12 static features from Iteration 1.\n",
    "* **New** 7 new dynamic features:\n",
    "  * `p1_fainted_count`, `p2_fainted_count`, `fainted_advantage`\n",
    "  * `p1_total_hp_pct`, `p2_total_hp_pct_estimated`, `hp_advantage`\n",
    "  * `p2_unseen_count`\n",
    "\n",
    "**Hypothesis:** Knowing how many Pokémon have fainted should be highly predictive.\n",
    "\n",
    "**Validation:** Simple 90/10 Split (to compare directly with Iteration 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240,
     "referenced_widgets": [
      "1ab6ab39cee144019dcdd0bb2e483f0b"
     ]
    },
    "id": "03a8b440",
    "outputId": "8f96fe5d-b8f4-4f4c-c385-511952f2c4f3"
   },
   "outputs": [],
   "source": [
    "# 1. Create v2 features\n",
    "X_v2, y_v2 = create_features(train_data, pokemon_db, \"v2\")\n",
    "\n",
    "# 2. Run v2 validation (using the same model as v1 for a fair comparison)\n",
    "results['v2_simple_split'] = run_simple_validation(\n",
    "    X_v2,\n",
    "    y_v2,\n",
    "    model_class=LogisticRegression,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "# 3. Analyze the improvement\n",
    "improvement = results['v2_simple_split'] - results['v1_simple_split']\n",
    "print(f\"\\nIteration 2 Accuracy: {results['v2_simple_split']:.4f}\")\n",
    "print(f\"Improvement from v1:  {improvement:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "71303cda"
   },
   "source": [
    "# 6. Iteration 3: Refinement (Granular Status & Boosts)\n",
    "\n",
    "**Objective:** Test if adding more subtle, granular dynamic features from the Turn 30 snapshot provides further lift. We will now model status conditions and stat boosts by splitting them into strategic groups.\n",
    "\n",
    "**Features (v3):** 34 features total.\n",
    "* **(v2)** All 19 features from Iteration 2 (Static + HP/Fainted).\n",
    "* **(New)** 15 new granular dynamic features:\n",
    "    * **Status (6 features):** Counts for strategic groups: `incapacitated` ('slp', 'frz'), `major_debuff` ('par', 'brn'), and `passive_damage` ('psn', 'tox').\n",
    "    * **Boosts (9 features):** Sums and \"advantage\" scores, separated into `offensive` (atk, spa), `defensive` (def, spd), and `speed` (spe).\n",
    "\n",
    "**Validation:**\n",
    "1.  **Simple 90/10 Split:** For a direct comparison with our Iteration 2 score.\n",
    "2.  **5-Fold K-Fold:** To establish our new, robust \"gold standard\" score for this feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483,
     "referenced_widgets": [
      "667aeea204f84dc3a507e32648d04168"
     ]
    },
    "id": "15c78ce8",
    "outputId": "c544a383-df6d-4ef5-d0c0-f17361e67d19"
   },
   "outputs": [],
   "source": [
    "# 1. Create v3 features\n",
    "X_v3, y_v3 = create_features(train_data, pokemon_db, \"v3\")\n",
    "\n",
    "print(\"\\n--- Running Validation 1: Simple 90/10 Split (for comparison) ---\")\n",
    "# 2. Run v3 validation (Simple Split)\n",
    "results['LR_v3_simple'] = run_simple_validation(\n",
    "    X_v3,\n",
    "    y_v3,\n",
    "    model_class=LogisticRegression,\n",
    "    max_iter=1000\n",
    ")\n",
    "improvement_v2_v3 = results['LR_v3_simple'] - results['v2_simple_split']\n",
    "print(f\"v3 Simple Split Accuracy: {results['LR_v3_simple']:.4f}\")\n",
    "print(f\"Improvement from v2:      {improvement_v2_v3:+.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Running Validation 2: Robust 5-Fold K-Fold (Gold Standard) ---\")\n",
    "# 3. Run v3 validation (K-Fold)\n",
    "results['LR_v3_kfold'] = run_kfold_validation(\n",
    "    X_v3,\n",
    "    y_v3,\n",
    "    n_splits=5,\n",
    "    model_class=LogisticRegression,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "print(f\"\\nIteration 3 Robust K-Fold Accuracy: {results['LR_v3_kfold']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "id": "e18f565b"
   },
   "source": [
    "# 7. Iteration 4: Model Experimentation\n",
    "\n",
    "**Objective:** Our \"v3_refined\" feature set (34 features) is our best performing set, achieving a **0.8480** K-Fold score using `LogisticRegression`.\n",
    "\n",
    "However, `LogisticRegression` is a linear model. It cannot capture complex, non-linear interactions between features (e.g., \"offensive boosts are only important if the opponent is not incapacitated\").\n",
    "\n",
    "We will now test more complex, non-parametric models (as mentioned in the competition PDF) on this same `X_v3` feature set to see if they can find these interactions and \"squeeze\" out a better score.\n",
    "\n",
    "**Models to Test:**\n",
    "\n",
    "* **k-Nearest Neighbors (kNN):** A simple instance-based model.\n",
    "* **Gradient Boosting (GB):** A powerful tree-based ensemble, excellent at finding complex relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d86af53d",
    "outputId": "b5c11a3d-004b-4343-c8d1-a06989224b21"
   },
   "outputs": [],
   "source": [
    "print(\"--- Testing Non-Parametric Models on 'v3_refined' Features ---\")\n",
    "\n",
    "# We assume X_v3 and y_v3 are loaded in memory from running Section 6.\n",
    "if 'X_v3' not in globals():\n",
    "    print(\"ERROR: X_v3 not found. Please re-run Section 6 first.\")\n",
    "else:\n",
    "    # 1. k-Nearest Neighbors (kNN)\n",
    "    # We will try a common default value for n_neighbors (e.g., 30)\n",
    "    print(\"\\nTesting KNeighborsClassifier...\")\n",
    "    results['kNN_v3_kfold'] = run_kfold_validation(\n",
    "        X_v3,\n",
    "        y_v3,\n",
    "        n_splits=5,\n",
    "        model_class=KNeighborsClassifier,\n",
    "        n_neighbors=30\n",
    "    )\n",
    "\n",
    "    # 2. Gradient Boosting Classifier (GB)\n",
    "    # We will start with balanced default hyperparameters\n",
    "    print(\"\\nTesting GradientBoostingClassifier...\")\n",
    "    results['GB_v3_kfold'] = run_kfold_validation(\n",
    "        X_v3,\n",
    "        y_v3,\n",
    "        n_splits=5,\n",
    "        model_class=GradientBoostingClassifier,\n",
    "        n_estimators=150,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Model Experimentation Results ---\")\n",
    "    print(f\"Logistic Regression (Baseline): {results.get('LR_v3_kfold', 0.0):.4f}\")\n",
    "    print(f\"k-Nearest Neighbors (kNN):    {results.get('kNN_v3_kfold', 0.0):.4f}\")\n",
    "    print(f\"Gradient Boosting (GB):       {results.get('GB_v3_kfold', 0.0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "f243a1d5"
   },
   "source": [
    "# 8. Analysis & Final Model Selection\n",
    "**Objective:** Review all robust K-Fold validation scores from our experiments to select the single best-performing model and feature set for our final submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "id": "29171cab"
   },
   "source": [
    "## 8.1. Results Summary\n",
    "Let's compile all K-Fold scores stored in our `results` dictionary into a DataFrame for a clear comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "771c58ee",
    "outputId": "e95de3a7-8abe-4491-c11a-5fb46cb2a3cd"
   },
   "outputs": [],
   "source": [
    "print(\"--- Final Results Summary (K-Fold Only) ---\")\n",
    "\n",
    "# Filter the 'results' dictionary to only include K-Fold scores\n",
    "kfold_results = {key: value for key, value in results.items() if 'kfold' in key}\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "results_df = pd.DataFrame.from_dict(kfold_results, orient='index', columns=['KFold_Accuracy'])\n",
    "results_df = results_df.sort_values(by='KFold_Accuracy', ascending=False)\n",
    "\n",
    "# Display the final comparison table\n",
    "display(results_df.style.format({'KFold_Accuracy': \"{:.4f}\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "id": "d8c893a4"
   },
   "source": [
    "## 8.2. Conclusion\n",
    "Based on the results, our analysis is very clear:\n",
    "\n",
    "* **k-Nearest Neighbors (kNN)** performed the worst (`~0.8299`), suggesting our 34-dimensional feature space is not well-suited for instance-based models.\n",
    "\n",
    "* **Gradient Boosting (GB)** (`~0.8466`) was surprisingly worse than our linear model. This indicates that the relationships in our data are primarily linear, and the complex interactions found by the tree model were likely just noise.\n",
    "\n",
    "* **Logistic Regression (LR)** on our `v3_refined` feature set is the undisputed winner, achieving the highest and most stable K-Fold score of 0.8480.\n",
    "\n",
    "**Final Model Selection:**\n",
    "\n",
    "* **Feature Set:** `v3_refined` (34 features)\n",
    "* **Model:** `LogisticRegression`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "73318ed6"
   },
   "source": [
    "# 9. Final Model Training & Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "id": "61fa03d4"
   },
   "source": [
    "## 9.1. Create Final Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347,
     "referenced_widgets": [
      "2934d832f9184916a1219137b07f69f1",
      "3e4fa0b0382846abb7d0093803915eea",
      "e416e4c684a0476d830dd9980e87ac45",
      "6067cc2f9728421cbda984eede88ebd3",
      "2ff9be47dae24178abcbbfdb6aeba785",
      "e3870496555446468baf54f5e9dfb69e",
      "fcfb047f83a74f9aa46539826c0af231",
      "18f1a8ac03df42d092e456520111dfbf",
      "548e65fd3f1a4897af3f9cb04bcf843d",
      "56ed91f9b06a4cdcb89f67a824174531",
      "7ebcee70c3c747899cb522ca1fd33602",
      "6d03ad1402164364af4dd038c1a6a554",
      "1fe555a007df44c383b24a35d8ec1d22",
      "286cbfc2a658486d96c2d9af8bbf6b76",
      "4f6e33cd3b224e59b1225d50ffb0b7e1",
      "198a923340894d30ac23df3effeb228f",
      "bb55bd76b2f54d65bfd0e7388e3e78bf",
      "e8e26f8f5c204d5fb567191483a6b8f5",
      "241237c8181c42efa59ed1470fd917a7",
      "ec2318e23d7c417a96b2d706ec633fdb",
      "163af2d7ebc7493f90e27a96a607b861",
      "2a4b68193a4041a491c4a7c3a0f52c21"
     ]
    },
    "id": "42df3ab7",
    "outputId": "46306486-09f3-4cc2-f7a8-5242bfa31bb8"
   },
   "outputs": [],
   "source": [
    "print(\"--- Starting Final Submission Process ---\")\n",
    "\n",
    "# 1. Define our final model configuration\n",
    "FINAL_MODEL = LogisticRegression(random_state=SEED, max_iter=1000)\n",
    "FINAL_FEATURES = \"v3\" # This is the name for our 'v3_refined' pipeline\n",
    "\n",
    "print(f\"Final Model: {FINAL_MODEL.__class__.__name__}\")\n",
    "print(f\"Final Feature Set: {FINAL_FEATURES} (34 features)\")\n",
    "\n",
    "# 2. Create features for 100% of training data\n",
    "print(\"Generating final training features...\")\n",
    "# We must use the 'v3' iteration name, which points to our refined functions\n",
    "X_train_final, y_train_final = create_features(train_data, pokemon_db, FINAL_FEATURES)\n",
    "\n",
    "# 3. Create features for 100% of test data\n",
    "print(\"\\nGenerating final test features...\")\n",
    "X_test_final, _ = create_features(test_data, pokemon_db, FINAL_FEATURES)\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train_final.shape}\")\n",
    "print(f\"Test set shape: {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "id": "5dd0b126"
   },
   "source": [
    "## 9.2. Align Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5362ac03",
    "outputId": "c681a5e4-6558-4f2c-96be-dde9a29e7b18"
   },
   "outputs": [],
   "source": [
    "# 4. Align columns\n",
    "print(\"\\nAligning training and test columns...\")\n",
    "X_test_final = X_test_final.reindex(columns=X_train_final.columns, fill_value=0)\n",
    "\n",
    "print(f\"Final feature count (aligned): {len(X_train_final.columns)}\")\n",
    "print(f\"Training set shape after align: {X_train_final.shape}\")\n",
    "print(f\"Test set shape after align: {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "id": "a14bc931"
   },
   "source": [
    "## 9.3. Train Final Model & Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "cde5d029",
    "outputId": "13d54c4f-b8ed-4263-fb71-951397d97f84"
   },
   "outputs": [],
   "source": [
    "# 5. Define and train the final pipeline on 100% of data\n",
    "print(f\"\\nTraining final model on 100% of training data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "final_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', FINAL_MODEL)\n",
    "])\n",
    "\n",
    "final_pipeline.fit(X_train_final, y_train_final)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Model training complete. Took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# 6. Generate predictions\n",
    "print(\"Generating predictions on test data...\")\n",
    "test_predictions = final_pipeline.predict(X_test_final)\n",
    "\n",
    "# 7. Create, save, and download the submission file\n",
    "print(\"Creating submission file...\")\n",
    "# Get the battle_id's from the original test_data list\n",
    "test_battle_ids = [battle[BATTLE_ID] for battle in test_data]\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    BATTLE_ID: test_battle_ids,\n",
    "    TARGET: test_predictions\n",
    "})\n",
    "\n",
    "# Ensure the target is an integer (0 or 1), not a boolean\n",
    "submission_df[TARGET] = submission_df[TARGET].astype(int)\n",
    "\n",
    "SUBMISSION_FILENAME = 'submission.csv'\n",
    "submission_df.to_csv(SUBMISSION_FILENAME, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully created '{SUBMISSION_FILENAME}'.\")\n",
    "display(submission_df.head())\n",
    "\n",
    "# 8. Trigger download (specific to Google Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"\\nTriggering file download...\")\n",
    "    files.download(SUBMISSION_FILENAME)\n",
    "except ImportError:\n",
    "    print(\"\\n(Skipping download: 'google.colab' not found. You can download the file manually.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "id": "OdOpz5ZzoNKD"
   },
   "source": [
    "# Second approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "id": "a2900918"
   },
   "source": [
    "## 1. Feature Engineering for Sequential Models\n",
    "\n",
    "This section defines the core helper functions (`_get_stats_from_db`, `extract_static_features_v1`, `_get_turn_30_snapshot`, `extract_dynamic_features_v2`, `extract_other_dynamic_features_v3`) for extracting features. It also includes the `create_features` (for non-sequential models) and introduces new functions: `extract_sequential_features` (per-turn feature extraction) and `create_sequential_features` (to build 3D feature tensors for sequential models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "op5AajOKfjuq",
    "outputId": "7558ebff-30e4-483a-bbcf-ff7aa837cf11"
   },
   "outputs": [],
   "source": [
    "from utils.sequential.feature import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "id": "c40c9a51"
   },
   "source": [
    "### 1.1. Generate Sequential Feature Tensors\n",
    "\n",
    "Using the `create_sequential_features` function, we generate 3D feature tensors for both the training and test datasets. These tensors are shaped `(n_battles, max_turns, n_features)`, which is suitable input for recurrent neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "llgQqAR2iabT",
    "outputId": "7647c0b9-3cbb-4853-8eb1-0bab479d7c27"
   },
   "outputs": [],
   "source": [
    "# New, sequential (per-turn)\n",
    "X_seq, y_seq, feature_names, lengths = create_sequential_features(\n",
    "    train_data, pokemon_db,\n",
    "    iteration_name='v3',   # includes all: v1 + v2 + v3 features\n",
    "    max_turns=30,\n",
    "    pad_value=0.0\n",
    ")\n",
    "\n",
    "X_seq_test, _, feature_cols_test, lengths_test = create_sequential_features(test_data, pokemon_db, 'v3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oqOX3nxRjL_9",
    "outputId": "25a5703a-33ed-49cc-ff20-75c0a0af1b99"
   },
   "outputs": [],
   "source": [
    "X_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {
    "id": "a2acd7dc"
   },
   "source": [
    "### 2.1. Setup PyTorch Environment and Libraries\n",
    "\n",
    "We import essential libraries from `torch`, `torch.nn`, `torch.utils.data`, `sklearn.model_selection`, and `matplotlib.pyplot` to build, train, and evaluate our sequential models using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "id": "JNGVWRtWle9f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {
    "id": "3b84930d"
   },
   "source": [
    "### 2.2. Prepare Data for PyTorch\n",
    "\n",
    "This step converts the NumPy feature arrays (`X_seq`, `y_seq`) into PyTorch tensors. We then split the training data into training and validation sets and create `TensorDataset` and `DataLoader` objects. `DataLoader`s handle batching and shuffling, which are crucial for efficient neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqgxKZySjq7H",
    "outputId": "ebec76cf-91c6-4184-d0c7-46ec06f0ff50"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert labels to numpy\n",
    "y_seq = y_seq.to_numpy().astype(np.float32)\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X_seq,\n",
    "    y_seq,\n",
    "    test_size=0.05,\n",
    "    random_state=42,\n",
    "    stratify=y_seq  # keep class balance\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation:\", X_validation.shape, y_validation.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "id": "5VQpltCBlvot"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "# Create Dataset objects\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "validation_dataset = TensorDataset(torch.tensor(X_validation, dtype=torch.float32), torch.tensor(y_validation, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(X_seq_test, dtype=torch.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "id": "lwE3pUTEmoe5"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {
    "id": "c5cc7bf1"
   },
   "source": [
    "### 2.3. Define LSTM Model Architecture\n",
    "\n",
    "We define the `LSTMClassifier` class, inheriting from `torch.nn.Module`. This model processes sequential input using an LSTM layer, optionally bidirectional, and includes a fully connected hidden layer followed by a dropout layer and a final linear layer for binary classification. The `forward` method describes the data flow through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {
    "id": "R4Ou1KydnO8-"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dim=64,\n",
    "        num_layers=1,\n",
    "        bidirectional=False,\n",
    "        dropout=0.0,\n",
    "        fc_hidden_dim=64   # NEW: size of additional FC layer\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.fc_hidden_dim = fc_hidden_dim\n",
    "\n",
    "        # -----------------------\n",
    "        # LSTM Encoder\n",
    "        # -----------------------\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "        lstm_output_dim = hidden_dim * self.num_directions\n",
    "\n",
    "        # -----------------------\n",
    "        # NEW: Fully Connected Hidden Layer\n",
    "        # -----------------------\n",
    "        self.fc1 = nn.Linear(lstm_output_dim, fc_hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        # -----------------------\n",
    "        # Final Binary Classifier Head\n",
    "        # -----------------------\n",
    "        self.fc2 = nn.Linear(fc_hidden_dim, 1)  # One output logit\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, D)\n",
    "        \"\"\"\n",
    "\n",
    "        x_out, (h_n, _) = self.lstm(x)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            # Combine forward/backward final states from last layer\n",
    "            h_forward = h_n[-2, :, :]\n",
    "            h_backward = h_n[-1, :, :]\n",
    "            h_last = torch.cat([h_forward, h_backward], dim=1)\n",
    "        else:\n",
    "            h_last = h_n[-1, :, :]  # (B, hidden_dim)\n",
    "\n",
    "        # NEW: Fully connected hidden layer\n",
    "        x = self.fc1(h_last)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Final classification\n",
    "        logits = self.fc2(x).squeeze(-1)  # (B,)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {
    "id": "0bb0c144"
   },
   "source": [
    "### 2.4. K-Fold Cross-Validation Training Utility\n",
    "\n",
    "The `kfold_train_for_config` function is designed to perform K-Fold Cross-Validation for a given set of hyperparameters. It initializes a model, trains it on `n_splits` folds, and tracks the mean validation loss per epoch, providing a robust measure of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "id": "WiKNdEF15Grw"
   },
   "outputs": [],
   "source": [
    "from utils.sequential.validation import kfold_train_for_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {
    "id": "63844844"
   },
   "source": [
    "### 2.5. Hyperparameter Tuning Configuration\n",
    "\n",
    "This section establishes a `base_config` with default hyperparameters for the LSTM model. It also defines grids of candidate values for various hyperparameters (`learning_rate`, `bidirectional`, `hidden_dim`, `fc_hidden_dim`, `num_layers`, `dropout`, `num_epochs`) that will be tuned. The computation `device` (CPU or GPU) is also set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "id": "25gRUIhb5P15"
   },
   "outputs": [],
   "source": [
    "base_config = {\n",
    "    \"hidden_dim\": 32,\n",
    "    \"fc_hidden_dim\": 32,\n",
    "    \"bidirectional\": False,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"num_epochs\": 20,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_layers\": 1,\n",
    "    \"batch_size\": 2048\n",
    "}\n",
    "\n",
    "\n",
    "# Example grids\n",
    "lr_grid            = [1e-4, 5e-4, 1e-3, 5e-3]\n",
    "bidirectional_grid = [False, True]\n",
    "hidden_dim_grid    = [32, 64, 128, 256]\n",
    "fc_hidden_grid     = [32, 64, 128, 256]\n",
    "num_layers_grid    = [1, 2, 3]\n",
    "dropout_grid       = [0.0, 0.2, 0.5]\n",
    "num_epochs_grid    = [20*i for i in range(1,5)]  # \"consider initial 20 epochs\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {
    "id": "b1f2a23f"
   },
   "source": [
    "### 2.6. General Training and Tuning Helper Functions\n",
    "\n",
    "Two essential helper functions are defined here:\n",
    "- `run_epoch`: Executes a single training or evaluation epoch, calculating loss and accuracy.\n",
    "- `tune_hyperparam`: Orchestrates the tuning process for a specific hyperparameter using K-Fold cross-validation, saves results to CSV, plots loss curves, and identifies the best parameter value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "id": "t_cWAvcv5bkT"
   },
   "outputs": [],
   "source": [
    "from utils.sequential.tune import tune_hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {
    "id": "yxm1UsBYJSyC"
   },
   "outputs": [],
   "source": [
    "def run_epoch(dataloader, model, criterion, optimizer=None):\n",
    "    \"\"\"\n",
    "    If optimizer is None -> evaluation mode (no backprop).\n",
    "    Returns: average loss, accuracy\n",
    "    \"\"\"\n",
    "    if optimizer is not None:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "\n",
    "    with torch.set_grad_enabled(optimizer is not None):\n",
    "        for seqs, labels in dataloader:\n",
    "            seqs = seqs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(seqs)          # (B,)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * seqs.size(0)\n",
    "\n",
    "            # Compute accuracy\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "            labels_int = labels.long()\n",
    "            total_correct += (preds == labels_int).sum().item()\n",
    "            total_examples += seqs.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_examples\n",
    "    acc = total_correct / total_examples\n",
    "    return avg_loss, acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {
    "id": "5d81bce1"
   },
   "source": [
    "### 2.7. Execute Hyperparameter Tuning Steps\n",
    "\n",
    "This block systematically tunes each hyperparameter one by one. For each parameter (`num_epochs`, `learning_rate`, `bidirectional`, `hidden_dim`, `fc_hidden_dim`, `num_layers`, `dropout`), the `tune_hyperparam` function is called. The best value found for each parameter is then updated in the `base_config` to be used in subsequent tuning steps, ensuring an iterative optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TrB0TV_i8eDf",
    "outputId": "13864bde-b5f4-42b4-ce77-17efa0522818"
   },
   "outputs": [],
   "source": [
    "# 5) Tune num_epochs (starting from 20 baseline)\n",
    "best_epochs, res_epochs = tune_hyperparam(\n",
    "    X=X_seq,\n",
    "    y=y_seq,\n",
    "    base_config=base_config,\n",
    "    param_name=\"num_epochs\",\n",
    "    values=num_epochs_grid,\n",
    "    results_csv_path=\"tuning_num_epochs_results.csv\",\n",
    "    plot_path=\"tuning_num_epochs_val_loss.png\"\n",
    ")\n",
    "base_config[\"num_epochs\"] = int(best_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gtecGkYz5reR",
    "outputId": "bd6e9d1e-641d-422c-8f2f-40079a12f43d"
   },
   "outputs": [],
   "source": [
    "# 1) Tune learning rate\n",
    "best_lr, res_lr = tune_hyperparam(\n",
    "    X=X_seq,\n",
    "    y=y_seq,\n",
    "    base_config=base_config,\n",
    "    param_name=\"learning_rate\",\n",
    "    values=lr_grid,\n",
    "    results_csv_path=\"tuning_lr_results.csv\",\n",
    "    plot_path=\"tuning_lr_val_loss.png\"\n",
    ")\n",
    "base_config[\"learning_rate\"] = best_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zoHL_Olu7fz_",
    "outputId": "f5fc0af8-c60d-4927-c6ae-1bf0589915ca"
   },
   "outputs": [],
   "source": [
    "# 2) Tune bidirectionality\n",
    "best_bi, res_bi = tune_hyperparam(\n",
    "    X=X_seq,\n",
    "    y=y_seq,\n",
    "    base_config=base_config,\n",
    "    param_name=\"bidirectional\",\n",
    "    values=bidirectional_grid,\n",
    "    results_csv_path=\"tuning_bidirectional_results.csv\",\n",
    "    plot_path=\"tuning_bidirectional_val_loss.png\"\n",
    ")\n",
    "base_config[\"bidirectional\"] = bool(best_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9PXZInJm7g4M",
    "outputId": "fcccd131-638f-4b62-8018-e1e66c0a8d95"
   },
   "outputs": [],
   "source": [
    "# 3) Tune hidden_dim\n",
    "best_hidden, res_hidden = tune_hyperparam(\n",
    "    X=X_seq,\n",
    "    y=y_seq,\n",
    "    base_config=base_config,\n",
    "    param_name=\"hidden_dim\",\n",
    "    values=hidden_dim_grid,\n",
    "    results_csv_path=\"tuning_hidden_dim_results.csv\",\n",
    "    plot_path=\"tuning_hidden_dim_val_loss.png\"\n",
    ")\n",
    "base_config[\"hidden_dim\"] = int(best_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKlYzeHc7mRo",
    "outputId": "a71c0d86-eefd-4684-ef67-297e9bf8d5ca"
   },
   "outputs": [],
   "source": [
    "# 4) Tune fc_hidden_dim\n",
    "best_fc_hidden, res_fc_hidden = tune_hyperparam(\n",
    "    X=X_seq,\n",
    "    y=y_seq,\n",
    "    base_config=base_config,\n",
    "    param_name=\"fc_hidden_dim\",\n",
    "    values=fc_hidden_grid,\n",
    "    results_csv_path=\"tuning_fc_hidden_dim_results.csv\",\n",
    "    plot_path=\"tuning_fc_hidden_dim_val_loss.png\"\n",
    ")\n",
    "base_config[\"fc_hidden_dim\"] = int(best_fc_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXTv7dbIW0aw",
    "outputId": "e2c75fe1-8817-4ff9-83db-b8a3c5986fd7"
   },
   "outputs": [],
   "source": [
    "# 6) Tune num_layers\n",
    "best_layers, _ = tune_hyperparam(\n",
    "    X=X_seq, y=y_seq,\n",
    "    base_config=base_config,\n",
    "    param_name=\"num_layers\",\n",
    "    values=num_layers_grid,\n",
    "    results_csv_path=\"tuning_num_layers_results.csv\",\n",
    "    plot_path=\"tuning_num_layers_plot.png\"\n",
    ")\n",
    "base_config[\"num_layers\"] = int(best_layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ujYpuCMGFQr",
    "outputId": "4451f129-4dbf-464b-a622-d7f9fd3f02f0"
   },
   "outputs": [],
   "source": [
    "# 7) Tune dropout\n",
    "best_dropout, _ = tune_hyperparam(\n",
    "    X=X_seq, y=y_seq,\n",
    "    base_config=base_config,\n",
    "    param_name=\"dropout\",\n",
    "    values=dropout_grid,\n",
    "    results_csv_path=\"tuning_dropout_results.csv\",\n",
    "    plot_path=\"tuning_dropout_plot.png\"\n",
    ")\n",
    "base_config[\"dropout\"] = float(best_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHjDMA7q8oxZ",
    "outputId": "345a2be4-0b3b-4872-97df-30e8eb6880f4"
   },
   "outputs": [],
   "source": [
    "print(\"\\nFinal chosen hyperparameters:\", base_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {
    "id": "4c963ec3"
   },
   "source": [
    "### 2.8. Final Model Training on Full Dataset\n",
    "\n",
    "After determining the optimal hyperparameters through the tuning process, the final LSTM model is instantiated with these best settings. It is then trained on the entire available training dataset (full_dataset) for a specified number of epochs. The training loss and accuracy are monitored and printed per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {
    "id": "OZrN9qWZBc_G"
   },
   "outputs": [],
   "source": [
    "full_dataset = TensorDataset(torch.tensor(X_seq, dtype=torch.float32), torch.tensor(y_seq, dtype=torch.float32))\n",
    "\n",
    "full_loader = DataLoader(\n",
    "    full_dataset,\n",
    "    batch_size=base_config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {
    "id": "DmJ6GH1YBnYu"
   },
   "outputs": [],
   "source": [
    "\n",
    "input_dim = X_seq.shape[2]\n",
    "\n",
    "model = LSTMClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=base_config[\"hidden_dim\"],\n",
    "    fc_hidden_dim=base_config[\"fc_hidden_dim\"],\n",
    "    bidirectional=base_config[\"bidirectional\"],\n",
    "    num_layers=base_config[\"num_layers\"],\n",
    "    dropout=base_config[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=base_config[\"learning_rate\"]\n",
    ")\n",
    "\n",
    "# num_epochs = base_config[\"num_epochs\"]\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PypHKdaCDI2W",
    "outputId": "bd039a36-7273-4c54-a047-81b49894213f"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_acc = run_epoch(full_loader, model, criterion, optimizer)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {
    "id": "l46o8O45DHh-"
   },
   "outputs": [],
   "source": [
    "# for epoch in range(1, num_epochs + 1):\n",
    "#     train_loss, train_acc = run_epoch(train_loader, model, criterion, optimizer)\n",
    "#     val_loss, val_acc = run_epoch(validation_loader, model, criterion, optimizer=None)\n",
    "#     if val_acc >= 0.89:\n",
    "#       break\n",
    "\n",
    "#     print(\n",
    "#         f\"Epoch {epoch:02d} | \"\n",
    "#         f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "#         f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {
    "id": "0_RZogkQq5qR"
   },
   "outputs": [],
   "source": [
    "def prediction(model, loader, test=False):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    # Initialize all_labels only if not in test mode, as it's not needed for test predictions\n",
    "    if not test:\n",
    "        all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      if test:\n",
    "          # For test data, batches contain only the sequence tensor\n",
    "          for batch_data in loader:\n",
    "              # Unpack the single sequence tensor from the batch tuple\n",
    "              seqs = batch_data[0].to(device)\n",
    "\n",
    "              logits = model(seqs)\n",
    "              probs = torch.sigmoid(logits)     # probability P(P1 wins)\n",
    "\n",
    "              # predicted class (0/1)\n",
    "              preds = (probs >= 0.5).long()\n",
    "\n",
    "              all_probs.extend(probs.cpu().numpy())\n",
    "              all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "          return all_probs, all_preds # Return only 2 values when test=True\n",
    "      else:\n",
    "          # For non-test data (e.g., train/validation), batches contain sequences and labels\n",
    "          for seqs_tensor, labels_tensor in loader:\n",
    "              seqs = seqs_tensor.to(device)\n",
    "              labels = labels_tensor.to(device)\n",
    "\n",
    "              logits = model(seqs)\n",
    "              probs = torch.sigmoid(logits)     # probability P(P1 wins)\n",
    "\n",
    "              # predicted class (0/1)\n",
    "              preds = (probs >= 0.5).long()\n",
    "              all_probs.extend(probs.cpu().numpy())\n",
    "              all_preds.extend(preds.cpu().numpy())\n",
    "              all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "          return all_probs, all_preds, all_labels # Return 3 values when test=False\n",
    "\n",
    "# Call the function correctly for test_loader, indicating it's test data (no labels)\n",
    "# The return will be only probabilities and predictions for the test set.\n",
    "all_probs, all_preds, all_labels = prediction(model, full_loader, test=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {
    "id": "0b8a3b4d"
   },
   "source": [
    "### 2.9. Model Prediction and Evaluation\n",
    "\n",
    "The `prediction` function facilitates generating probabilities and predicted labels from the trained model. This block then uses this function to predict on the training data (`full_loader`) and evaluates the model's performance using standard classification metrics: Confusion Matrix, Classification Report, and ROC AUC Score. The ROC curve is also plotted for visual analysis of the model's discriminative power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "id": "4Qm9MH-oxBFA",
    "outputId": "35f24409-2508-4a1c-de55-727f2fef2df2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Confusion matrix & classification report\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "report = classification_report(all_labels, all_preds)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "# --- ROC & AUC ---\n",
    "fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "auc_score = roc_auc_score(all_labels, all_probs)\n",
    "\n",
    "print(f\"\\nROC AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.4f}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line for random performance\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XECjEkkQxAlv",
    "outputId": "8de8b50d-e8a9-4ca5-ba0d-ef15f707c4fd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(all_preds, all_probs)\n",
    "# Example: maximize Youden’s J statistic (tpr - fpr)\n",
    "best_idx = np.argmax(tpr - fpr)\n",
    "thresholds[best_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {
    "id": "255c63e8"
   },
   "source": [
    "### 2.10. Generate Submission File\n",
    "\n",
    "In this final step, the trained LSTM model is used to make predictions on the `test_loader` (unseen test data). The predicted `player_won` labels are then compiled into a pandas DataFrame along with their corresponding `battle_id`s and saved to `submission.csv`, ready for submission to the competition. The distribution of the predicted labels is also displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {
    "id": "eTzi2Wxp1Bhj"
   },
   "outputs": [],
   "source": [
    "all_probs, all_preds = prediction(model, test_loader, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {
    "id": "Zw8vhkAUsBWd"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([(index, label) for index, label in enumerate(all_preds)], columns=['battle_id','player_won']).to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "9yU8YJSN1HAI",
    "outputId": "8226d7b7-2207-4f1a-c842-22d5b566e80b"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([(index, label) for index, label in enumerate(all_preds)], columns=['battle_id','player_won'])['player_won'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {
    "id": "guhEMlzp3S-Q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
